# LogEvaluation
Execute file main.py to create output files. Two types of metrics are used - Single Model Metrics and Data Set Metrics. First only shows resutls for a single model with a singel Selection Criteria, while second one compares results for all used Models and Selection Criteria of a data set. 
Each Metric as a defined "MOI", which specifies the Extractor, that has to be used in order to get results. If the Extractor is not applied, the metric won't be either.

Output is either in .txt format, to derive a table or as a Figure, store as pdf.

WARNING: Potentially, to many figures will be created, if all extractors + metric for all data sets etc. are activated. 

## Root
Defines, where to find output files. Output files are required to have structure, generated by Experiment runner.

----

## Extractor
Extraction functions used to derive dictionary of results for Metric appliance onto logged experiment output. They require specific EvaluationCriteria to be present in the logged file. 

----
## Singel Model Metric
### WeightedMcc
Measurement for: Weighted Matthew Correlation Coefficient.\
Requires Extractor "WeightedMccExtractor"


### RelativeCertaintyCorrectnessEval
Measurement for: missclassification ratio by predictive distinctiveness.\
Requires Extractor "CertaintyCorrectnessEval"

### CertaintyCorrectnessEval
Measurement for: Misclassification ratio by uncertainty group.\
Requires Extractor "UncertaintyMisclassificationCorrelation"

----
## DataSet Metric
### AverageOutlierSamplingRatio
Measurement for: Outlier Sampling Ratio.\
Requires Extractor "SampledLabel"

### AverageNearestNeighborSamplePointDistance
Measurement for: Distance between two sampled points.\
Requires Extractor "SampledPoints"

### AverageBestLearningCurvePlotWithStd
Measurement for: MCC of best learning Results.\
Requires Extractor "MatthewCorrelationCoefficientExtractor"

### WeightedLogger
Measurement for: Logging weighted MatthewCorrelationCoefficient.\
Requires Extractor "WeightedMccExtractor"

Logs End Values in Table 

---
